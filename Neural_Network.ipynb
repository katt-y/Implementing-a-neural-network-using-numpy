{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "data=scipy.io.loadmat('train_.mat')\n",
    "X=data['X']\n",
    "Y=data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 9, 2, ..., 1, 6, 9], dtype=uint8)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in range(73257):\n",
    "    j=X[:,:,:,i].reshape(1024,3)\n",
    "    l.append([j])\n",
    "\n",
    "X_2= np.concatenate(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73257, 1024, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_3 = X_2.reshape(73257,3072)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(73257, 3072)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_3.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=scipy.io.loadmat('test_.mat')\n",
    "X_test=data['X']\n",
    "Y_test=data['y']\n",
    "Y_test =Y_test.flatten()\n",
    "\n",
    "l=[]\n",
    "for i in range(26032):\n",
    "    j=X_test[:,:,:,i].reshape(1024,3)\n",
    "    l.append([j])\n",
    "\n",
    "X_test = np.concatenate(l)\n",
    "X_test = X_test.reshape(26032,3072)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def  loss_func(soft_max,true_value) :\n",
    "\n",
    "   m = -np.log(soft_max[true_value-1])\n",
    "   \n",
    "   return m\n",
    "    # m = list(zip(predicted_value,true_value))\n",
    "    # L = 0\n",
    "    # for i,j in m:\n",
    "    #     g = j*(np.log(i)) + (1-j)*(1-np.log(1-i))\n",
    "    #     L += g\n",
    "\n",
    "\n",
    "    # L = (1/len(predicted_value))*L\n",
    "\n",
    "    # return L\n",
    "\n",
    "def sigmoid(x):\n",
    "\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def diff_sigm(x):\n",
    "\n",
    "    return (x)*(1-x)\n",
    "\n",
    "def compute_softmax(x):\n",
    "   \n",
    "   l = np.vectorize(np.exp)(x)\n",
    "\n",
    "   s = np.sum(l)\n",
    "\n",
    "   l = l/s\n",
    "\n",
    "   return l \n",
    "   \n",
    "def compute_gradient(p,x,y):\n",
    "   \n",
    "   weights = p.weights\n",
    "   weights = np.flip(weights,0)\n",
    "   output = p.predict(x)\n",
    "   softmax = output[2]\n",
    "   output_for_layers = output[1]\n",
    "   # output_for_layers = np.flip(output_for_layers,axis=0)\n",
    "   gradient_wrt_output = []\n",
    "   gradient_wrt_weights = []\n",
    "   gradients = []\n",
    "   output_with_weights = []\n",
    "   \n",
    "   softmax[y - 1] = softmax[y - 1] - 1\n",
    "   gradient_wrt_output = softmax\n",
    "   \n",
    "  \n",
    "   tuple_output_for_layers = [(np.array(output_for_layers[i]),np.array(output_for_layers[i+1])) for i in range(len(output_for_layers)-1)]\n",
    "   tuple_output_for_layers.reverse()\n",
    "\n",
    "   \n",
    "   i = 0\n",
    "   gradient_layer = gradient_wrt_output\n",
    "\n",
    "   while i != len(weights) :\n",
    "      gradients = []\n",
    "    \n",
    "      m = [gradient_layer,np.vectorize(diff_sigm)(tuple_output_for_layers[i][1]),np.append(tuple_output_for_layers[i][0],1)]\n",
    "\n",
    "      for t in range(len(gradient_layer)):\n",
    "         k = (m[0][t]*m[1][t])*m[2]\n",
    "         gradients.append(k)\n",
    "      gradients = np.array(gradients,dtype ='float64')   \n",
    "      \n",
    "      gradient_wrt_weights.append(gradients)\n",
    "      gradient_layer_2 = []\n",
    "      j = (np.array(weights[i])).transpose()\n",
    "      p = np.multiply(gradient_layer,m[1])\n",
    "      for k in j :\n",
    "\n",
    "         n = np.sum(np.multiply(p,k))\n",
    "         gradient_layer_2.append(n)\n",
    "      gradient_layer_2.pop()\n",
    "      # l = []\n",
    "      # for g in gradient_layer_2:\n",
    "\n",
    "      #    f = g.astype('float64')\n",
    "      #    l.append(f)\n",
    "\n",
    "      # gradient_layer_2 = np.array(l)\n",
    "\n",
    "      # gradient_layer_2 = np.array(gradient_layer_2)\n",
    "\n",
    "      gradient_layer = gradient_layer_2\n",
    "\n",
    "      i+=1\n",
    "\n",
    "\n",
    "   gradient_wrt_weights = np.array(gradient_wrt_weights)\n",
    "   gradient_wrt_weights = np.flip(gradient_wrt_weights,axis = 0)\n",
    "\n",
    "   return gradient_wrt_weights\n",
    "\n",
    "\n",
    "   # for j in range(len(gradient_wrt_output)):\n",
    "\n",
    "   # for i in range(len(weights)):\n",
    "      \n",
    "   #    d = list(zip(weights[i],output_for_layers[i]))\n",
    "   #    output_with_weights.append(d)\n",
    "\n",
    "   # output_with_weights = np.flip(output_with_weights,axis = 0)  \n",
    "\n",
    "\n",
    "   # for i in output_with_weights:\n",
    "   #    layer_gradient = []\n",
    "   #    # for j in range(len(i)-1):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network :\n",
    "\n",
    "    def __init__(self,num_of_layers=3,input_size=1000,output_size=10,number_neurons=[0],classes=[]):\n",
    "        \n",
    "        if len(number_neurons)!= num_of_layers :\n",
    "            raise ValueError('length of hidden_neurons should be equal to the number of hidden layers')\n",
    "        \n",
    "        \n",
    "        self.num_of_layers = num_of_layers\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.number_neurons = number_neurons\n",
    "        self.classes = classes\n",
    "    \n",
    "    \n",
    "    def weight_initialization(self,technique='Xavier'):\n",
    "        \n",
    "        if technique == 'Xavier':\n",
    "            number_neurons = self.number_neurons\n",
    "            \n",
    "            weights = []\n",
    "            for i in range(1,len(number_neurons)):\n",
    "               \n",
    "                if i != len(number_neurons)-1:\n",
    "                    layer_weight=[]\n",
    "                    var = 2 / (number_neurons[i-1] + (number_neurons[i+1])) # \n",
    "                    sigma = np.sqrt(var)\n",
    "                else:\n",
    "                    layer_weight=[]\n",
    "                    var = 2 / (number_neurons[i-1]) # \n",
    "                    sigma = np.sqrt(var)\n",
    "\n",
    "            \n",
    "                \n",
    "                for b in range(number_neurons[i]):\n",
    "                    l = np.random.normal(0,sigma,number_neurons[i-1]+1)\n",
    "                    # l = l.astype('float64')\n",
    "                    layer_weight.append(l)\n",
    "                # layer_weight = np.array(layer_weight,dtype = 'float64')\n",
    "                weights.append(layer_weight)\n",
    "            \n",
    "            weights=np.array(weights)\n",
    "            self.weights = weights\n",
    "            return weights \n",
    "            \n",
    "    def predict(self,x):\n",
    "        y = self.weights\n",
    "        i = 0\n",
    "        L = x\n",
    "        output_array = []\n",
    "        output_array.append(x)\n",
    "        while len(L)!= 10:\n",
    "            M=[]\n",
    "            for j in y[i]:\n",
    "                k_1 = np.dot(j[:-1], L)\n",
    "                k = k_1 + j[-1]\n",
    "                k = 1/(1 + np.exp(-k))                               # Sigmoid activation function \n",
    "                M.append(k) \n",
    "                \n",
    "\n",
    "            output_array.append(M)\n",
    "            L = np.array(M)\n",
    "            i+=1\n",
    "\n",
    "\n",
    "            \n",
    "        L = compute_softmax(L)\n",
    "\n",
    "       \n",
    "        \n",
    "        \n",
    "        m = np.max(L)\n",
    "\n",
    "        class_label = np.where(L == m)\n",
    "\n",
    "        class_label =  class_label[0][0] + 1\n",
    "\n",
    "        output_array = np.array(output_array)\n",
    "        predictions = (class_label,output_array,L)\n",
    "    \n",
    "        return predictions\n",
    "\n",
    "    def test(self,test_data, kind = 'Macro'):\n",
    "         \n",
    "        Classes_dict = {}\n",
    "\n",
    "        for i in range(1,11):\n",
    "            Classes_dict[i] = [0,0,0]\n",
    "\n",
    "        Input,Output = (test_data[0],test_data[1])\n",
    "\n",
    "\n",
    "        for j in range(len(Input)):\n",
    "\n",
    "            d = self.predict(Input[j])\n",
    "            d = d[0]\n",
    "\n",
    "            if d == Output[j]:\n",
    "                Classes_dict[d][0]+=1\n",
    "            else:\n",
    "                Classes_dict[Output[j]][2]+=1\n",
    "                Classes_dict[d][1]+=1\n",
    "\n",
    "        if kind == 'Macro':\n",
    "\n",
    "                l = []\n",
    "\n",
    "                for i in Classes_dict:\n",
    "                    if Classes_dict[i][0] != 0:\n",
    "                        k = Classes_dict[i][0]/(Classes_dict[i][0]+Classes_dict[i][1])\n",
    "                        l.append(k)\n",
    "                    else:\n",
    "                        l.append(0)    \n",
    "\n",
    "                l = np.array(l)\n",
    "\n",
    "                precision = np.sum(l)/10\n",
    "\n",
    "                l = []\n",
    "                for i in Classes_dict:\n",
    "                    if Classes_dict[i][0] != 0:\n",
    "                        k = Classes_dict[i][0]/(Classes_dict[i][0]+Classes_dict[i][2])\n",
    "                        l.append(k)\n",
    "                    else :\n",
    "                        l.append(0)\n",
    "\n",
    "                l = np.array(l)\n",
    "\n",
    "                recall = np.sum(l)/10\n",
    "\n",
    "\n",
    "                f_1_score = 2*precision*recall/(precision + recall)\n",
    "                \n",
    "                m = 0\n",
    "                for i in Classes_dict:\n",
    "                    m+=Classes_dict[i][0]\n",
    "\n",
    "                accuracy = m/len(Input)\n",
    "\n",
    "\n",
    "\n",
    "                return (f_1_score,precision,recall,accuracy)\n",
    "\n",
    "        elif kind == 'Micro':\n",
    "\n",
    "                l = []\n",
    "                t_p = 0\n",
    "                f_p = 0\n",
    "\n",
    "                for i in Classes_dict:\n",
    "                   t_p += Classes_dict[i][0]\n",
    "                   f_p += Classes_dict[i][1]\n",
    "                   \n",
    "                precision = t_p/(t_p + f_p)\n",
    "                \n",
    "\n",
    "                l = []\n",
    "                t_p  = 0\n",
    "                f_p = 0\n",
    "\n",
    "                for i in Classes_dict:\n",
    "                   t_p += Classes_dict[i][0]\n",
    "                   f_n += Classes_dict[i][2]\n",
    "                   \n",
    "                recall = t_p/(t_p + f_n) \n",
    "\n",
    "\n",
    "                f_1_score = 2*precision*recall/(precision + recall)\n",
    "\n",
    "\n",
    "                m = 0\n",
    "                for i in Classes_dict:\n",
    "                    m+=Classes_dict[i][0]\n",
    "\n",
    "                accuracy = m/len(Input)\n",
    "\n",
    "\n",
    "\n",
    "                return (f_1_score,precision,recall,accuracy)\n",
    "\n",
    "    # def compute_gradient(self,x,y):\n",
    "    #     gradient = []\n",
    "    #     rev_array = np.flip(x)\n",
    "    #     for i in x:\n",
    "\n",
    "    #         for j in i:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimizers : \n",
    "\n",
    "    def __init__(self) :\n",
    "        self.weights = []\n",
    "        self.index = 0\n",
    "        self.epoch = 0\n",
    "\n",
    "\n",
    "    # def SGD(self,neural_network,training_data,batch_size,epochs,learning_rate):\n",
    "\n",
    "    #     number_of_batches = len(training_data[0]) // batch_size\n",
    "    #     for f in range(epochs):\n",
    "    #         gradient = np.zeros(neural_network.weights.shape, dtype = 'O' )\n",
    "    #         for k in range(number_of_batches):\n",
    "    #             i,j = (training_data[0][(k-1)*batch_size:k*batch_size],training_data[1][(k-1)*batch_size:k*batch_size])\n",
    "    #             weight_gradient = np.zeros(neural_network.weights.shape,dtype = 'O' )\n",
    "    #             while len(i) != 0:\n",
    "\n",
    "    #                 Input = i[-1]\n",
    "    #                 Output = j[-1]\n",
    "    #                 i = np.delete(i,-1,0)\n",
    "    #                 j = np.delete(j,-1)\n",
    "    #                 d =  compute_gradient(neural_network,Input,Output)\n",
    "    #                 # d = d.astype('float64')\n",
    "    #                 weight_gradient += d\n",
    "    #             weight_gradient = (1/(batch_size))*weight_gradient\n",
    "\n",
    "    #             gradient += weight_gradient\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    #         gradient = (1/number_of_batches)*gradient\n",
    "\n",
    "    #         neural_network.weights -= learning_rate*gradient\n",
    "\n",
    "\n",
    "    #     return None\n",
    "    \n",
    "    def SGD(self,neural_network,training_data,epochs,learning_rate):\n",
    "\n",
    "        l = [(training_data[0][i],training_data[1][i]) for i in range(len(training_data[0])-1000)]\n",
    "        l = np.array(l)\n",
    "        i = 0\n",
    "        validation_set = (training_data[0][-1000:],training_data[1][-1000:])\n",
    "        while i!= epochs:\n",
    "            np.random.shuffle(l)\n",
    "            self.epoch +=1\n",
    "\n",
    "            for j in l:\n",
    "                self.index +=1\n",
    "                d = compute_gradient(neural_network,j[0],j[1])\n",
    "                neural_network.weights-= learning_rate*d\n",
    "                if self.index % 5000 == 0:\n",
    "                    stopping_criteria = neural_network.test(validation_set)[0]\n",
    "                    self.weights.append(stopping_criteria)\n",
    "           \n",
    "                    if stopping_criteria >= .9 :\n",
    "                        return None\n",
    "                    \n",
    "            \n",
    "            i+=1\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def RMSPROP(self,neural_network,training_data,epochs,learning_rate,rho,epsilon):\n",
    "\n",
    "        l = [(training_data[0][i],training_data[1][i]) for i in range(len(training_data[0])-1000)]\n",
    "        l = np.array(l)\n",
    "        Lambda = learning_rate\n",
    "        i = 0\n",
    "        v_h = 0\n",
    "        d_2 = 0\n",
    "        validation_set = (training_data[0][-1000:],training_data[1][-1000:])\n",
    "        while i!= epochs:\n",
    "            np.random.shuffle(l)\n",
    "            \n",
    "            for j in l:\n",
    "                self.index +=1\n",
    "                \n",
    "                d = compute_gradient(neural_network,j[0],j[1])\n",
    "                d = np.square(d)\n",
    "                d_2 = d_2*rho + d*(1-rho)\n",
    "                learning_rate = Lambda/(d_2 + epsilon)\n",
    "                neural_network.weights-= learning_rate*v_h\n",
    "                if self.index % 5000 == 0:\n",
    "                    stopping_criteria = neural_network.test(validation_set)[0]\n",
    "                    self.weights.append(stopping_criteria)\n",
    "                    if stopping_criteria >= .9:\n",
    "                        return None\n",
    "                    \n",
    "            \n",
    "            i+=1\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementing the neural network for Stochastic Gradient Descent and RMSProp. You may test the network by running these codes . Implementing the optimizers took about 250 minutes for each RMSProp and SGD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Neural_Network(3,3072,10,[3072,1500,10])\n",
    "weights = m.weight_initialization()\n",
    "#f = m.predict(X_3[0])\n",
    "#gr = compute_gradient(m,X_3[0], Y[0])\n",
    "\n",
    "sd = optimizers()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the SGD optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd.SGD(m,(X_3,Y),5,.1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Testing the weights obtained by SGD optimizer on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = m.test((X_test,Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.03120555596167358,\n",
       " 0.018421793359413543,\n",
       " 0.10196154089123825,\n",
       " 0.07767363245236632)"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## f_1_score = 0.03120555596167358\n",
    "## precision = 0.018421793359413543\n",
    "## recall = 0.10196154089123825\n",
    "## accuracy = 0.07767363245236632"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the RMSProp optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Neural_Network(3,3072,10,[3072,1500,10])\n",
    "weights = m.weight_initialization()\n",
    "sd = optimizers()\n",
    "sd.RMSPROP(m,(X_3,Y),5,.001,.9,1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/85/tt6k5_h12qsgwhr9kvl_f4hr0000gn/T/ipykernel_8602/3576540119.py:78: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  output_array = np.array(output_array)\n",
      "/var/folders/85/tt6k5_h12qsgwhr9kvl_f4hr0000gn/T/ipykernel_8602/3576540119.py:57: RuntimeWarning: overflow encountered in exp\n",
      "  k = 1/(1 + np.exp(-k))                               # Sigmoid activation function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.05343012075465881,\n",
       " 0.03568537371432808,\n",
       " 0.10627687992054129,\n",
       " 0.08163030116779349)"
      ]
     },
     "execution_count": 435,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.test((X_test,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
